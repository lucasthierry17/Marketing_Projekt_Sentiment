{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube Sentiment Analysis\n",
    "\n",
    "In the following sentiment analysis, we look at a YouTube data set with videos that were trending in the period from 13.09.2017 - 22.10.2017. We compare the trends from the UK and US and analyze the behavior of viewers from those regions.\n",
    "\n",
    "The data set can be accessed via the following link\n",
    "https://www.kaggle.com/datasets/datasnaek/youtube\n",
    "\n",
    "Our data set consists of two csv and one json file for both regions. One of the two csv files contains all comments for each video that was trending, while the other csv file lists all videos in general. \n",
    "\n",
    "The json file contains information on all possible categories to which a video could belong. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Start by importing all the necessary librarys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the necessary imports\n",
    "# !pip install openpyxl \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the json File\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_dataframe(file):\n",
    "    \"\"\"Function that reads a JSON file, extracts category_id and title from each item, and converts it into a Pandas DataFrame.\"\"\"\n",
    "    \n",
    "    with open(file, 'r', encoding='utf-8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    # Extracting relevant information from JSON\n",
    "    categories = []\n",
    "    for item in data['items']:\n",
    "        category_id = item['id']\n",
    "        title = item['snippet']['title']\n",
    "        categories.append({'category_id': category_id, 'title': title})\n",
    "    \n",
    "    # Creating DataFrame\n",
    "    df = pd.DataFrame(categories)\n",
    "    return df\n",
    "\n",
    "cwd = os.getcwd()\n",
    "filename_gb = \"GB_category_id.json\"\n",
    "filename_us = \"US_category_id.json\"\n",
    "\n",
    "# JSON-Datei in DataFrame umwandeln\n",
    "gb_categories = json_to_dataframe(os.path.join(cwd, filename_gb))\n",
    "us_categories = json_to_dataframe(os.path.join(cwd, filename_us))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Json File category contains all possible categories and their corresponding ID. \n",
    "\n",
    "This DataFrame becomes relevant in the course of the sentiment analysis, as the category of the video is stored as an ID in the csv file.\n",
    "\n",
    "For illustrative purposes, we want to exchange the ID with the corresponding category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading both csv Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to CSV files\n",
    "csv_file_videos_gb = (os.path.join(cwd, \"GBvideos.csv\"))\n",
    "csv_file_comments_gb = (os.path.join(cwd, \"GBcomments.csv\"))\n",
    "\n",
    "csv_file_videos_us = (os.path.join(cwd, \"USvideos.csv\"))\n",
    "csv_file_comments_us = (os.path.join(cwd, \"UScomments.csv\"))\n",
    "\n",
    "# load the files\n",
    "gb_videos = pd.read_csv(csv_file_videos_gb, delimiter=\",\", error_bad_lines=False) \n",
    "gb_comments = pd.read_csv(csv_file_comments_gb, delimiter=\",\", error_bad_lines=False)\n",
    "\n",
    "us_videos = pd.read_csv(csv_file_videos_us, delimiter=\",\", error_bad_lines=False)\n",
    "us_comments = pd.read_csv(csv_file_comments_us, delimiter=\",\", error_bad_lines=False)\n",
    "\n",
    "\n",
    "\n",
    "# Display the first 5 rows of the DataFrame\n",
    "gb_videos.head()\n",
    "gb_videos = gb_videos.iloc[:, :11]\n",
    "print(gb_videos.columns)\n",
    "gb_videos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete emojis from the comments\n",
    "\n",
    "### To perform the sentiment analysis properly, we need to remove special characters such as emojis from the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    \"\"\"The function remove_emojis takes a text input and removes any emojis present in the text.\"\"\"\n",
    "    # Check if the input is a string\n",
    "    if isinstance(text, str):\n",
    "        # Define regex pattern to match emojis\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U00002702-\\U000027B0\"\n",
    "                                   u\"\\U000024C2-\\U0001F251\"\n",
    "                                   u\"\\U0001f926-\\U0001f937\"\n",
    "                                   u\"\\U00010000-\\U0010ffff\"\n",
    "                                   u\"\\u2640-\\u2642\" \n",
    "                                   u\"\\u2600-\\u2B55\"\n",
    "                                   u\"\\u200d\"\n",
    "                                   u\"\\u23cf\"\n",
    "                                   u\"\\u23e9\"\n",
    "                                   u\"\\u231a\"\n",
    "                                   u\"\\ufe0f\"  # dingbats\n",
    "                                   u\"\\u3030\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "        # Remove emojis from the text\n",
    "        return emoji_pattern.sub(r'', text)\n",
    "    else:\n",
    "        # If input is not a string, return it unchanged\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the remove_emojis function to each element in the \"comment_text\" column\n",
    "gb_comments[\"comment_text\"] = gb_comments[\"comment_text\"].apply(remove_emojis)\n",
    "us_comments[\"comment_text\"] = us_comments[\"comment_text\"].apply(remove_emojis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping\n",
    "### Mapping the category_Id to category_name for so we can adjust our dataset like previously described"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping dictionary from category_id to category_name\n",
    "category_mapping_gb = dict(zip(gb_categories['category_id'], gb_categories['title']))\n",
    "category_mapping_us = dict(zip(us_categories['category_id'], us_categories['title']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the category_id to category_name in your main dataframe\n",
    "gb_videos['category_id'] = gb_videos['category_id'].astype(str)\n",
    "gb_videos['category_name'] = gb_videos['category_id'].map(category_mapping_gb)\n",
    "\n",
    "us_videos['category_id'] = us_videos['category_id'].astype(str)\n",
    "us_videos['category_name'] = us_videos['category_id'].map(category_mapping_us)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Data\n",
    "\n",
    "Now that we have completely loaded and processed all the data, it is time to analyze it. To do this, we start with a few plots that give us an insight into the performance of the individual channels and videos.\n",
    "\n",
    "We then look at the use of tags and their frequency within our data set. Finally, we perform the sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from wordcloud import WordCloud, STOPWORDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot the top channels\n",
    "def plot_top_channels(videos_df, country, color_palette, channel_color_dict):\n",
    "    top_channels = videos_df[\"channel_title\"].value_counts().head(10)\n",
    "    \n",
    "    # Create a DataFrame for plotting\n",
    "    top_channels_df = pd.DataFrame({'channel': top_channels.index, 'appearances': top_channels.values})\n",
    "    \n",
    "    # Plot using seaborn\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    sns.barplot(x='appearances', y='channel', data=top_channels_df, palette=[channel_color_dict[channel] for channel in top_channels_df['channel']])\n",
    "    \n",
    "    plt.xlabel(\"Appearances\", fontsize=16)\n",
    "    plt.ylabel(\"\", fontsize=16)  # Remove the y-axis label\n",
    "    plt.title(f\"Top 10 Channels in YouTube Trends of {country}\", fontsize=20)\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    \n",
    "    # Add annotation to bars\n",
    "    for index, value in enumerate(top_channels_df['appearances']):\n",
    "        plt.text(value + 0.2, index, str(int(value)), fontsize=16, fontweight=\"bold\", color=\"grey\")\n",
    "        plt.text(value / 2, index, top_channels_df['channel'][index], fontsize=16, fontweight=\"bold\", color=\"white\", ha='center')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Get the top 10 channels for both UK and US\n",
    "top_uk_channels = gb_videos[\"channel_title\"].value_counts().head(10).index\n",
    "top_us_channels = us_videos[\"channel_title\"].value_counts().head(10).index\n",
    "\n",
    "# Combine and get unique channels\n",
    "unique_channels = pd.Series(list(top_uk_channels) + list(top_us_channels)).unique()\n",
    "\n",
    "# Create a color palette\n",
    "color_palette = sns.color_palette(\"husl\", len(unique_channels))\n",
    "channel_color_dict = {channel: color_palette[i] for i, channel in enumerate(unique_channels)}\n",
    "\n",
    "# Plot for UK\n",
    "plot_top_channels(gb_videos, \"UK\", color_palette, channel_color_dict)\n",
    "\n",
    "# Plot for US\n",
    "plot_top_channels(us_videos, \"US\", color_palette, channel_color_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot the top channels\n",
    "def plot_top_videos(videos_df, country, color_palette, video_color_dict):\n",
    "    top_videos = videos_df[\"video_id\"].value_counts().head(10)\n",
    "    \n",
    "    # Create a DataFrame for plotting\n",
    "    top_videos_df = pd.DataFrame({'video_id': top_videos.index, 'appearances': top_videos.values})\n",
    "    top_videos_df = top_videos_df.merge(videos_df[['video_id', 'title']].drop_duplicates(), on='video_id', how='left')\n",
    "    \n",
    "    # Plot using seaborn\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    sns.barplot(x='appearances', y='video_id', data=top_videos_df, palette=[video_color_dict[video] for video in top_videos_df['video_id']])\n",
    "    \n",
    "    plt.xlabel(\"Appearances\", fontsize=16)\n",
    "    plt.ylabel(\"\", fontsize=16)  # Remove the y-axis label\n",
    "    plt.title(f\"Top 10 Videos in YouTube Trends of {country}\", fontsize=20)\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    \n",
    "    # Add annotation to bars\n",
    "    for index, value in enumerate(top_videos_df['appearances']):\n",
    "        plt.text(value + 0.2, index, str(int(value)), fontsize=16, fontweight=\"bold\", color=\"grey\")\n",
    "        plt.text(value / 2, index, top_videos_df['title'][index], fontsize=16, fontweight=\"bold\", color=\"white\", ha='center', wrap=True)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Get the top 10 videos for both UK and US\n",
    "top_uk_videos = gb_videos[\"video_id\"].value_counts().head(10).index\n",
    "top_us_videos = us_videos[\"video_id\"].value_counts().head(10).index\n",
    "\n",
    "# Combine and get unique videos\n",
    "unique_videos = pd.Series(list(top_uk_videos) + list(top_us_videos)).unique()\n",
    "\n",
    "# Create a color palette\n",
    "color_palette = sns.color_palette(\"husl\", len(unique_videos))\n",
    "video_color_dict = {video: color_palette[i] for i, video in enumerate(unique_videos)}\n",
    "\n",
    "# Plot for UK\n",
    "plot_top_videos(gb_videos, \"UK\", color_palette, video_color_dict)\n",
    "\n",
    "# Plot for US\n",
    "plot_top_videos(us_videos, \"US\", color_palette, video_color_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking which Tags are represented the most in UK\n",
    "tags = gb_videos['tags'].map(lambda k: k.lower().split('|')).values \n",
    "k= (' '.join(gb_videos['tags']))  \n",
    "wordcloud = WordCloud(width = 1000, height = 500).generate((' '.join(k.lower().split('|'))))\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking which Tags are represented the most in UK\n",
    "tags = us_videos['tags'].map(lambda k: k.lower().split('|')).values \n",
    "k= (' '.join(gb_videos['tags']))  \n",
    "wordcloud = WordCloud(width = 1000, height = 500).generate((' '.join(k.lower().split('|'))))\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on the plot, the three most frequently appearing categories in YouTube trends are Entertainment, Music, and People & Blogs.\n",
    "\n",
    "If we compare both the performing channels and the videos from the USA with those from the UK in relation to the appearances in the trends, it is noticeable that categories such as entertainment and comedy are trending well. \n",
    "\n",
    "This also makes sense, as YouTube is a platform for entertainment and is therefore also used for this purpose. What is interesting, however, is that we can see exact overlaps in some places. For example, the video \"Eminem Rips Donald Trump in BET Hip Hop Awards Freestyle Cypher\" is trending multiple times in both regions. There are also similarities in the channels. \"The Late Late Show with James Cordon\" or \"jacksfilms\" are on average around 38 days with their videos in the trends in the USA and the UK. \n",
    "\n",
    "This suggests that viewers from the USA and the UK have the same interests when consuming YouTube videos. The USA could even be seen as a trendsetter that other countries follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique categories\n",
    "categories_gb = gb_videos[\"category_name\"].unique()\n",
    "categories_us = us_videos[\"category_name\"].unique()\n",
    "\n",
    "# Create seperate DataFrames for each category and store them in variables\n",
    "category_dataframes_gb = {}\n",
    "for category in categories_gb:\n",
    "    category_dataframes_gb[category] = gb_videos[gb_videos[\"category_name\"] == category].copy()\n",
    "\n",
    "category_dataframes_us = {}\n",
    "for category in categories_us:\n",
    "    category_dataframes_us[category] = us_videos[us_videos[\"category_name\"] == category].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DataFrame for each Category\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "science_and_technology_gb = pd.DataFrame(category_dataframes_gb[\"Science & Technology\"])\n",
    "entertainment_gb = pd.DataFrame(category_dataframes_gb[\"Entertainment\"])\n",
    "film_and_animation_gb = pd.DataFrame(category_dataframes_gb[\"Film & Animation\"])\n",
    "howto_and_style_gb = pd.DataFrame(category_dataframes_gb[\"Howto & Style\"])\n",
    "sports_gb = pd.DataFrame(category_dataframes_gb[\"Sports\"])\n",
    "people_and_blogs_gb = pd.DataFrame(category_dataframes_gb[\"People & Blogs\"])\n",
    "music_gb = pd.DataFrame(category_dataframes_gb[\"Music\"])\n",
    "comedy_gb = pd.DataFrame(category_dataframes_gb[\"Comedy\"])\n",
    "education_gb = pd.DataFrame(category_dataframes_gb[\"Education\"])\n",
    "news_and_politics_gb = pd.DataFrame(category_dataframes_gb[\"News & Politics\"])\n",
    "gaming_gb = pd.DataFrame(category_dataframes_gb[\"Gaming\"])\n",
    "autos_and_vehicles_gb = pd.DataFrame(category_dataframes_gb[\"Autos & Vehicles\"])\n",
    "pets_and_animals_gb = pd.DataFrame(category_dataframes_gb[\"Pets & Animals\"])\n",
    "travel_and_events_gb = pd.DataFrame(category_dataframes_gb[\"Travel & Events\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "science_and_technology_us = pd.DataFrame(category_dataframes_us[\"Science & Technology\"])\n",
    "entertainment_us = pd.DataFrame(category_dataframes_us[\"Entertainment\"])\n",
    "film_and_animation_us = pd.DataFrame(category_dataframes_us[\"Film & Animation\"])\n",
    "howto_and_style_us = pd.DataFrame(category_dataframes_us[\"Howto & Style\"])\n",
    "sports_us = pd.DataFrame(category_dataframes_us[\"Sports\"])\n",
    "people_and_blogs_us = pd.DataFrame(category_dataframes_us[\"People & Blogs\"])\n",
    "music_us = pd.DataFrame(category_dataframes_us[\"Music\"])\n",
    "comedy_us = pd.DataFrame(category_dataframes_us[\"Comedy\"])\n",
    "education_us = pd.DataFrame(category_dataframes_us[\"Education\"])\n",
    "news_and_politics_us = pd.DataFrame(category_dataframes_us[\"News & Politics\"])\n",
    "gaming_us = pd.DataFrame(category_dataframes_us[\"Gaming\"])\n",
    "autos_and_vehicles_us = pd.DataFrame(category_dataframes_us[\"Autos & Vehicles\"])\n",
    "pets_and_animals_us = pd.DataFrame(category_dataframes_us[\"Pets & Animals\"])\n",
    "travel_and_events_us = pd.DataFrame(category_dataframes_us[\"Travel & Events\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dataframes = [entertainment_gb, music_gb, people_and_blogs_gb, howto_and_style_gb]\n",
    "category_names = ['Entertainment', 'Music', 'People & Blogs', 'Howto & Style']\n",
    "\n",
    "# Create bar plots for the amount of views for each category separately\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, category_df in enumerate(category_dataframes):\n",
    "    # Calculate the total views for each category\n",
    "    total_views = category_df['views'].sum()\n",
    "    plt.bar(category_names[i], total_views, color='skyblue')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Total Views by Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Total Views')\n",
    "\n",
    "# Show the plot\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dataframes = [entertainment_us, music_us, people_and_blogs_us, howto_and_style_us]\n",
    "category_names = ['Entertainment', 'Music', 'People & Blogs', 'Howto & Style']\n",
    "\n",
    "# Create bar plots for the amount of views for each category separately\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, category_df in enumerate(category_dataframes):\n",
    "    # Calculate the total views for each category\n",
    "    total_views = category_df['views'].sum()\n",
    "    plt.bar(category_names[i], total_views, color='skyblue')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Total Views by Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Total Views')\n",
    "\n",
    "# Show the plot\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentimentIntensityAnalyzer\n",
    "In our sentiment analysis, we want to analyze the mood of the comments in the various categories. To do this, we create a separate dataframe for each category. We then look at the \"comment_text\" column to carry out the sentiment analysis accordingly and plot the results.\n",
    "\n",
    "The SentimentIntensityAnalyzer is a component of the VADER (Valence Aware Dictionary and sEntiment Reasoner) sentiment analysis tool. VADER is specifically attuned to sentiments expressed in social media. It works well on text from various domains, including social media posts, news articles, and even longer narrative texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_compound(category, comments, csv_name):\n",
    "    \"\"\"\n",
    "    \n",
    "    The add_compound function integrates sentiment analysis into a dataset by:\n",
    "\n",
    "    1. Merging categories and comments based on video_id.\n",
    "    2. Computing sentiment scores for each comment using VADER.\n",
    "    3. Adding these scores to the original dataset.\n",
    "    4. Saving the enhanced dataset to a CSV file and returning it.\n",
    "    5. The primary output is a DataFrame that includes sentiment scores (including the \"compound\" score) for each comment,\n",
    "    providing insight into the emotional tone of the comments.\n",
    "\n",
    "    \"\"\"\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    category_comments = pd.merge(category, comments[[\"video_id\", \"comment_text\"]], on=\"video_id\", how=\"left\")\n",
    "    #print(category_comments.head())\n",
    "    category_comments = category_comments.reset_index()\n",
    "    category_comments = category_comments.rename(columns={'index': 'ID'})\n",
    "\n",
    "    results = {}\n",
    "    for i, row in tqdm(category_comments.iterrows(), total=len(category_comments)):\n",
    "        text = row[\"comment_text\"]\n",
    "        myid = i\n",
    "        results[myid] = sia.polarity_scores(str(text))\n",
    "\n",
    "    vaders = pd.DataFrame(results).T\n",
    "    vaders = vaders.reset_index().rename(columns={\"index\": \"ID\"})\n",
    "    #vaders = vaders.drop([\"neg\", \"neu\", \"pos\"], axis=1)\n",
    "    \n",
    "    category_comments = vaders.merge(category_comments, how=\"left\")\n",
    "    #category_comments = category_comments.drop([\"ID\"], axis=1)\n",
    "    # Add print statement to check if the \"compound\" column exists\n",
    "    print(\"Columns after merging:\", category_comments.columns)\n",
    "    category_comments.to_csv(csv_name, index=False)\n",
    "\n",
    "\n",
    "    return category_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the Compound Values for every video from UK\n",
    "gb_videos_comments = add_compound(gb_videos, gb_comments,\"gb_videos_comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the Compound Values for every video from US\n",
    "us_videos_commments = add_compound(us_videos, us_comments, \"us_video_comments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "science_and_technology_comments_gb = add_compound(science_and_technology_gb, gb_comments,\"science_and_technology_comments_gb.csv\" )\n",
    "entertainment_comments_gb = add_compound(entertainment_gb, gb_comments,\"entertainment_comments_gb.csv\")\n",
    "film_and_animation_comments_gb = add_compound(film_and_animation_gb, gb_comments, \"film_and_animation_comments_gb.csv\")\n",
    "howto_and_style_comments_gb = add_compound(howto_and_style_gb, gb_comments, \"howto_and_style_comments_gb.csv\")\n",
    "sports_comments_gb = add_compound(sports_gb, gb_comments, \"sports_comments_gb.csv\")\n",
    "people_and_blogs_comments_gb = add_compound(people_and_blogs_gb, gb_comments, \"people_and_blogs_comments_gb.csv\")\n",
    "music_comments_gb = add_compound(music_gb, gb_comments, \"music_comments_gb.csv\")\n",
    "comedy_comments_gb = add_compound(comedy_gb, gb_comments, \"comedy_comments_gb.csv\")\n",
    "education_comments_gb = add_compound(education_gb, gb_comments, \"education_comments_gb.csv\")\n",
    "news_and_politics_comments_gb = add_compound(news_and_politics_gb, gb_comments, \"news_and_politics_comments_gb.csv\")\n",
    "gaming_comments_gb = add_compound(gaming_gb, gb_comments, \"gaming_comments_gb.csv\")\n",
    "autos_and_vehicles_comments_gb = add_compound(autos_and_vehicles_gb, gb_comments, \"autos_and_vehicles_comments_gb.csv\")\n",
    "pets_and_animals_comments_gb = add_compound(pets_and_animals_gb, gb_comments, \"pets_and_animals_comments_gb.csv\")\n",
    "travel_and_events_comments_gb = add_compound(travel_and_events_gb, gb_comments, \"travel_and_events_comments_gb.csv\") \n",
    "\n",
    "# Dauer ca. 11min (entspannte drei Clash Royale Runden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "science_and_technology_comments_us = add_compound(science_and_technology_us, us_comments,\"science_and_technology_comments_us.csv\" )\n",
    "entertainment_comments_us = add_compound(entertainment_us, us_comments,\"entertainment_comments_us.csv\")\n",
    "film_and_animation_comments_us = add_compound(film_and_animation_us, us_comments, \"film_and_animation_comments_us.csv\")\n",
    "howto_and_style_comments_us = add_compound(howto_and_style_us, us_comments, \"howto_and_style_comments_us.csv\")\n",
    "sports_comments_us = add_compound(sports_us, us_comments, \"sports_comments_us.csv\")\n",
    "people_and_blogs_comments_us = add_compound(people_and_blogs_us, us_comments, \"people_and_blogs_comments_us.csv\")\n",
    "music_comments_us = add_compound(music_us, us_comments, \"music_comments_us.csv\")\n",
    "comedy_comments_us = add_compound(comedy_us, us_comments, \"comedy_comments_us.csv\")\n",
    "education_comments_us = add_compound(education_us, us_comments, \"education_comments_us.csv\")\n",
    "news_and_politics_comments_us = add_compound(news_and_politics_us, us_comments, \"news_and_politics_comments_us.csv\")\n",
    "gaming_comments_us = add_compound(gaming_us, us_comments, \"gaming_comments_us.csv\")\n",
    "autos_and_vehicles_comments_us = add_compound(autos_and_vehicles_us, us_comments, \"autos_and_vehicles_comments_us.csv\")\n",
    "pets_and_animals_comments_us = add_compound(pets_and_animals_us, us_comments, \"pets_and_animals_comments_us.csv\")\n",
    "travel_and_events_comments_us = add_compound(travel_and_events_us, us_comments, \"travel_and_events_comments_us.csv\") \n",
    "\n",
    "# Dauer ca. 11min (entspannte drei Clash Royale Runden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"compound\"\n",
    "\n",
    "dict_of_compounds_gb = {\"Entertainment\": entertainment_comments_gb[metric],\n",
    "                     \"Science and Technology\": science_and_technology_comments_gb[metric],\n",
    "                     \"Film and Animation\": film_and_animation_comments_gb[metric],\n",
    "                     \"Howto and Style\": howto_and_style_comments_gb[metric],\n",
    "                     \"Sports\": sports_comments_gb[metric],\n",
    "                     \"People and Blogs\": people_and_blogs_comments_gb[metric],\n",
    "                     \"Music\": music_comments_gb[metric],\n",
    "                     \"Comedy\": comedy_comments_gb[metric],\n",
    "                     \"Education\": education_comments_gb[metric],\n",
    "                     \"News and Politics\": news_and_politics_comments_gb[metric],\n",
    "                     \"Gaming\": gaming_comments_gb[metric],\n",
    "                     \"Autos and vehicles\": autos_and_vehicles_comments_gb[metric],\n",
    "                     \"Pets and Animals\": pets_and_animals_comments_gb[metric],\n",
    "                     \"Travel and Events\": travel_and_events_comments_gb[metric]}\n",
    "len(dict_of_compounds_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"compound\"\n",
    "\n",
    "dict_of_compounds_us = {\"Entertainment\": entertainment_comments_us[metric],\n",
    "                     \"Science and Technology\": science_and_technology_comments_us[metric],\n",
    "                     \"Film and Animation\": film_and_animation_comments_us[metric],\n",
    "                     \"Howto and Style\": howto_and_style_comments_us[metric],\n",
    "                     \"Sports\": sports_comments_us[metric],\n",
    "                     \"People and Blogs\": people_and_blogs_comments_us[metric],\n",
    "                     \"Music\": music_comments_us[metric],\n",
    "                     \"Comedy\": comedy_comments_us[metric],\n",
    "                     \"Education\": education_comments_us[metric],\n",
    "                     \"News and Politics\": news_and_politics_comments_us[metric],\n",
    "                     \"Gaming\": gaming_comments_us[metric],\n",
    "                     \"Autos and vehicles\": autos_and_vehicles_comments_us[metric],\n",
    "                     \"Pets and Animals\": pets_and_animals_comments_us[metric],\n",
    "                     \"Travel and Events\": travel_and_events_comments_us[metric]}\n",
    "len(dict_of_compounds_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"compound\"\n",
    "dict_of_compounds = {\"sports\": sports_comments_gb[metric], \"entertainment\": entertainment_comments_gb[metric]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VADERS Results UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [-1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "\n",
    "num_rows = 2  # Number of rows of plots\n",
    "num_cols = 7  # Number of columns of plots\n",
    "\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(20, 5))\n",
    "\n",
    "# Iterate through the dictionary and plot each metric for different categories\n",
    "for i, (category, df) in enumerate(dict_of_compounds_gb.items()):\n",
    "    ax = axes[i // num_cols, i % num_cols]  # Determine the corresponding axis\n",
    "    ax.hist(df, bins=bins, color=\"skyblue\", edgecolor=\"black\")  # Plot histogram for the metric\n",
    "    ax.set_title(category)  # Set the title for each plot\n",
    "\n",
    "    median_value = df.median()\n",
    "    ax.axvline(median_value, color='red', linestyle='--', linewidth=2, label=f'Median: {median_value:.2f}')\n",
    "    ax.legend()  # Show legend\n",
    "\n",
    "plt.tight_layout()  # Optimize the arrangement of plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VADERS RESULT US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [-1, -0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "\n",
    "num_rows = 2  # Number of rows of plots\n",
    "num_cols = 7  # Number of columns of plots\n",
    "\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(20, 5))\n",
    "\n",
    "# Iterate through the dictionary and plot each metric for different categories\n",
    "for i, (category, df) in enumerate(dict_of_compounds_us.items()):\n",
    "    ax = axes[i // num_cols, i % num_cols]  # Determine the corresponding axis\n",
    "    ax.hist(df, bins=bins, color=\"skyblue\", edgecolor=\"black\")  # Plot histogram for the metric\n",
    "    ax.set_title(category)  # Set the title for each plot\n",
    "\n",
    "    median_value = df.median()\n",
    "    ax.axvline(median_value, color='red', linestyle='--', linewidth=2, label=f'Median: {median_value:.2f}')\n",
    "    ax.legend()  # Show legend\n",
    "\n",
    "plt.tight_layout()  # Optimize the arrangement of plots\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
